---
title: "Client Report - Star Wars for Dummies"
subtitle: "Unit 5 Task 3"
author: "Ezekial Curran"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
import polars as pl
import numpy as np
from lets_plot import *
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
  classification_report, 
  accuracy_score, 
  recall_score, 
  precision_score, 
  f1_score
  )
# add the additional libraries you need to import for ML here

LetsPlot.setup_html(isolated_frame=True)
```


```{python}
# import your data here using pandas and the URL
url = "https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv"
df = pl.read_csv("StarWars.csv")

df_clean = df.rename({
    df.columns[1]: "seen",
    df.columns[2]: "fan",
    **{df.columns[i]: f"seen_epi_{['i', 'ii', 'iii', 'iv', 'v', 'vi'][i - 3]}" for i in range(3, 9)},
    **{df.columns[i]: f"rank_epi_{['i', 'ii', 'iii', 'iv', 'v', 'vi'][i - 9]}" for i in range(9, 15)},
    **{df.columns[i]: df[df.columns[i]][0].lower().replace(' ', '_') for i in range(15, 29)},
    df.columns[29]: "shot_first",
    df.columns[30]: "ex_uni",
    df.columns[31]: "fan_ex_uni",
    df.columns[32]: "fan_star_trek",
    **{df.columns[i]: df.columns[i].lower().replace(' ', '_') for i in range(33, 37)},
    df.columns[37]: "location"
})

df_clean = df_clean[1:]
```

## QUESTION 1

1. __Prep the data for machine learning:__
    a. Create your target (also known as “y” or “label”) column based on the new income range column  
    a. One-hot encode all remaining categorical columns 

The "y" or "label" column is the encoded income range and the "X" group is everything except for the income and respondent ID columns. Some columns were manually encoded like age and rankings. The education and income were also manually encoded, but it was on a linear scale, which may introduce issues on accurately representing the data. The rest were encoded using One Hot encoding.

```{python}
# Include and execute your code here

df_clean = df_clean.with_columns(
  pl.col("age").replace({
    "> 60": 4,
    "45-60": 3,
    "30-44": 2,
    "18-29": 1,
  }, default=0).alias("age_num")
)

df_clean = df_clean.with_columns(
  pl.col(df_clean.columns[i]).replace({
    "Very favorably": 6,
    "Somewhat favorably": 5,
    "Neither favorably nor unfavorably (neutral)": 4,
    "Somewhat unfavorably": 3,
    "Very unfavorably": 2,
    "Unfamiliar (N/A)": 1
  }, default=0).alias(df_clean.columns[i]) for i in range(15, 29)
)

df_clean = df_clean.with_columns(
  pl.when((pl.col(df_clean.columns[i]).is_null()))
  .then(pl.lit("no"))
  .otherwise(pl.lit("yes"))
  .alias(df_clean.columns[i]) for i in range(3, 9)
)

# It may be worthwhile to just one-hot encode these two groups since they are ordinal and the numeric representation implies the distance between these values as being equal and that is not the case.
df_clean = df_clean.with_columns(
  pl.col('education').replace({
    "Graduate degree": 5,
    "Bachelor degree": 4,
    "Some college or Associate degree": 3,
    "High school degree": 2,
    "Less than high school degree": 1
  }, default=0).alias("education_num")
)

df_clean = df_clean.with_columns(
  pl.col('household_income').replace({
    "$150,000+": 5,
    "$100,000 - $149,999": 4,
    "$50,000 - $99,999": 3,
    "$25,000 - $49,999": 2,
    "$0 - $24,999": 1
  }, default=0).alias("income")
)

cat_cols = ['rank_epi_i', 'rank_epi_ii', 'rank_epi_iii', 'rank_epi_iv', 'rank_epi_v', 'rank_epi_vi', 'seen', 'fan', 'seen_epi_i', 'seen_epi_ii', 'seen_epi_iii', 'seen_epi_iv', 'seen_epi_v', 'seen_epi_vi', 'shot_first', 'ex_uni', 'fan_ex_uni', 'fan_star_trek', 'gender', 'location']
df_clean = df_clean.drop(['age', 'education', 'household_income'])

enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
enc_ar = enc.fit_transform(df_clean[cat_cols])
enc_cols = list(enc.get_feature_names_out(cat_cols))
enc_df = pl.DataFrame(enc_ar, schema=enc_cols)
df_tot = pl.concat([df_clean.drop(cat_cols), enc_df], how='horizontal')


X = df_tot.drop(['RespondentID', 'income'])
y = df_tot.select('income')

display(df_clean)
```


## QUESTION 2

1. __Build a machine learning model that predicts whether a person makes at least $50k. Describe your model and report the accuracy.__

The classification of 3, 4, and 5 are all income ranges that are at least $50k. The model learned this very poorly. The accuracy based on the test is about 39%. This is very likely due to the significant amount of missing data present in the dataset. 

```{python}
# Include and execute your code here
rnd = 343
X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, random_state=rnd, test_size=0.2)

model = GradientBoostingClassifier()
model.fit(X_trn, y_trn)
pred = model.predict(X_tst)
print(classification_report(y_tst, pred))
```