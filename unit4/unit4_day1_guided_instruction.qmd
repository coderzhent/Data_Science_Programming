---
title: "Machine Learning with Flights Primer"
format: html
---

This activity is meant to build upon slides shared by the instructor.

You will be using the scikit-learn library. If you have not installed it yet, you may need to install it 

```{python}
# %% 
# install packages
#run pip install scikit-learn in the terminal
# Alternatively, the code below worked for me before, but not anymore!

# import sys
# !{sys.executable} -m pip install --upgrade scikit-learn
```

This tutorial will help to provide a concrete example of walking through the workflow of a machine learning model using scikit-learn.

The first step is to load the necessary libraries.

```{python}
import sklearn
import polars as pl
import pandas as pd
import numpy as np
from lets_plot import *

LetsPlot.setup_html(isolated_frame=True)
```

Let's use the flights data since we are already familiar with it. We can try to predict what airport a month's information is from based on the information.

We'll first read in and clean the data. This is something you should be familiar with by now. Fill in the **** with the correct command or function to make the code run.

## Prep the data

```{python}
df = pd.read_json("https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json")

df = df.replace({"n/a": np.nan,
            "1500+": 1500,
            -999.0: np.nan})

df.num_of_delays_carrier = df.num_of_delays_carrier.astype(int)
```

Many machine learning models cannot handle missing data very well. Some of them will throw out the whole row of data if any of the columns have a missing value. Our dataset is small, but you can imagine if you have 50 or 100 columns, it would not be surprising for each row to have at least 1 missing value. You might end up throwing out most of your data. One way to impute the missing values is by replacing missing values with the mean for that variable. 

*Note: you could have a whole course about missing values. This is a very basic approach to fixing the missing data so that we can get a model to run.

```{python}
fillvalues = {'minutes_delayed_nas': df.minutes_delayed_nas.mean(), 
              'num_of_delays_late_aircraft': df.num_of_delays_late_aircraft.mean(),
              'minutes_delayed_carrier': df.minutes_delayed_carrier.mean()}
df_cleaned = df.fillna(value = fillvalues)
df_cleaned
```

## Partition the data

Now the dataset is cleaned, I can partition it according to: feature variables (x) and the target variable (y).

The features are considered the model inputs, or x's. They are the information I anticipate having and using to make a prediction. The target will contain the true, or actual, result observed - commonly thought of the as the y variable.

Many/most machine learning models cannot receive categorical variables as inputs. In the next unit we will learn how to handle categorical variables, but for now we will simply exclude them.

```{python}
# write code on the right side of the equal sign that will return only the numeric variables
x = df_cleaned.drop(columns=['airport_code', 'month', 'airport_name', 'year'])

y = df_cleaned.airport_code
```

Run the following code chunk just to verify the number of rows and columns in each object, x and y, match your expectations.
```{python}
print(x.shape)
print(y.shape)
```

We will now do additional partitions to split the x and y objects into a training set and a test set.

"Training set" is the data given to the machine learning algorithm to learn from. It will estimate parameters and learn relationships that help it make good predictions.

The "test" dataset, also called the "hold out" dataset, is something we set aside to assess how well our model performed. After we see how well the model performed, we are not allowed to go back and fix/adjust the model (that would defeat the purpose of having a "hold out" dataset).

Think of the gospel context, when something is consecrated, it is set apart for a holy purpose. You should consider your "test" dataset sacred. :)

Splitting a dataset by these two dimensions creates 4 separate partitions:

- x train
- x test
- y train
- y test

Fill in the **** with the correct command. If you aren't sure what commands to use, reference the [classifiers section](https://byuidatascience.github.io/DS250-Course-Draft/Course%20Materials/ml.html#classifiers) of the assigned reading Machine Learning Introduction.

The command below takes the x and y objects and splits it into 4. It puts 20% of the data into the test group, and 80% into the training set. Common values for the percent of data in the test set range from 10-30%.

```{python}
from sklearn.model_selection import train_test_split



# You can name the 4 objects anything you like. These names are pretty standard though
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# You can use these two commands to check the size of the 4 objects and verify they meet your expectations
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
```

Note the use of `random_state = 42` in the code above. This sets the random seed for the function to create the split. This way, all the students will get the same results, and you will get the same result each time you run it. Otherwise, the functions creates a different random sample of points each time it is run, which can be difficult to reproduce.

## Fit the model

Now you need to pick a machine learning model. We'll use the Decision Tree model since that is what the reading focused on. Most of the models you will want to use for this class are in sklearn.tree or sklearn.ensemble. 

Get curious: what is an ensemble model? How is a random forest different from a decision tree?

```{python}
from sklearn.tree import DecisionTreeClassifier #import the model
from sklearn.ensemble import RandomForestClassifier #example of a different model to try
from sklearn.ensemble import GradientBoostingClassifier

my_classifier_Dec = DecisionTreeClassifier() #define the model
my_classifier_Ran = RandomForestClassifier()
my_classifier_XGB = GradientBoostingClassifier()
my_classifier_Dec.fit(x_train, y_train) #fit the model on the training data
my_classifier_Ran.fit(x_train, y_train)
my_classifier_XGB.fit(x_train, y_train)
```

At first, the DecisionTreeClassifier() command seems a bit odd since it is such an empty step. Because we accepted all the defaults there is nothing in the parenthesis. This steps sets up the model structure, or defines the algorithm, but hasn't given it any data yet.

Here's an example where some of the defaults are altered:

`my_classifier = RandomForestClassifier(criterion = "entropy", max_depth = 2, n_estimators = 400)`

You can begin to see, it is important to define what model you actually want before training the model. In other words, in this step you are setting some hyperparameters which define the algorithm. When you fit the model, the model then estimates parameters specific to your dataset.

## Make predictions

Use the model to make predictions. At this point, we feed the model the x values of the test set, but don't give it the answers.


```{python}
pred_Dec = my_classifier_Dec.predict(x_test)
pred_Ran = my_classifier_Ran.predict(x_test)
pred_XGB = my_classifier_XGB.predict(x_test)
```

## Evaluate the model performance

We will learn about various model performance metrics later. For now, let's just calculate the model accuracy: the proportion of predictions that are correct. 

```{python}
from sklearn.metrics import classification_report
print(classification_report(y_test, pred_Dec))
```


```{python}
print(classification_report(y_test, pred_Ran))
```


```{python}
print(classification_report(y_test, pred_XGB))
```