---
title: "Client Report - Show me!"
subtitle: "Unit 4 Task 3"
author: "Ezekial Curran"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
import torch
import shap
import torch.nn as nn
import torch.optim as optim
import polars as pl
import numpy as np
from lets_plot import *
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.inspection import permutation_importance
from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin
from sklearn.metrics import (
  classification_report,
  accuracy_score,
  recall_score,
  precision_score,
  f1_score,
  r2_score,
  median_absolute_error,
  mean_absolute_error,
  mean_squared_error,
  root_mean_squared_error
  )

LetsPlot.setup_html(isolated_frame=True)
```


```{python}
# import your data here using pandas and the URL
url1 = "https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv"
url2 = "https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv"
url3 = "https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_denver/dwellings_denver.csv"
url4 = "https://github.com/byuidatascience/data4dwellings/blob/master/data.md"

df = pl.read_csv('dwellings_ml.csv')
```

## Supplimental Code

```{python}
class Neural_Net(nn.Module):
  def __init__(self, input_size, shallow=True):
    super(Neural_Net, self).__init__()
    if shallow:
      self.net = nn.Sequential(
        nn.Linear(input_size, 32),
        nn.ReLU(),
        nn.Linear(32, 1)
      )
    else:
      self.net = nn.Sequential(
        nn.Linear(input_size, 32),
        nn.ReLU(),
        nn.Dropout(0.1),

        nn.Linear(32, 16),
        nn.ReLU(),
        nn.Dropout(0.1),

        nn.Linear(16, 1)
      )
  
  def forward(self, x):
    return self.net(x)

class TorchModelWrapper(BaseEstimator):
  def __init__(self, model, device, task="classification", scorer="f1_score"):
    self.model = model
    self.device = device
    self.task = task # 'classification' or 'regression'
    self.scorer = scorer

  def fit(self, X, y):
    return self

  def predict(self, X):
    self.model.eval()
    with torch.no_grad():
      X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)
      outputs = self.model(X_tensor)

      if self.task == "regression":
        return outputs.cpu().numpy().flatten()
      elif self.task == "classification":
        if outputs.shape[1] == 1:
          probs = torch.sigmoid(outputs).cpu().numpy()
          return (probs >= 0.5).astype(int).flatten()
        else:
          return torch.argmax(outputs, dim=1).cpu().numpy()
      else:
        raise ValueError(f"Unsupported task type: {self.task}\nOnly two types of tasks are available: regression and classification (default)")

  def score(self, X, y):
    y_pred = self.predict(X)

    if self.scorer == "f1_score":
      return f1_score(y, y_pred)
    elif self.scorer == "accuracy_score":
      return accuracy_score(y, y_pred)
    elif self.scorer == "meanAE":
      return mean_absolute_error(y, y_pred)
    elif self.scorer == "medAE":
      return median_absolute_error(y, y_pred)
    elif self.scorer == "MSE":
      return mean_squared_error(y, y_pred)
    elif self.scorer == "RMSE":
      return root_mean_squared_error(y, y_pred)
    elif self.scorer == "r2_score":
      return r2_score(y, y_pred)
    elif self.scorer == "recall_score":
      return recall_score(y, y_pred)
    elif self.scorer == "precision_score":
      return precision_score(y, y_pred)
    else:
      raise ValueError(f"Unsupported scoring method: {self.scorer}")

def train_model(model, name, X_train, X_test, y_train, y_test, epochs=1000):
  criterion = nn.BCEWithLogitsLoss()
  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
  train_losses = []
  test_accuracies = []
  f1_scores = []
  # r2_scores = [] # Only for regression tasks not classification
  # rmses = [] # Only for regression tasks not classification

  y_true = y_test.int().cpu().numpy() if torch.cuda.is_available() else y_test

  print(f"Training: {name}")
  for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
    train_losses.append(loss.item())

    model.eval()
    with torch.no_grad():
      probs = torch.sigmoid(model(X_test))
      preds = (probs >=0.5).int().cpu().numpy() if torch.cuda.is_available() else (probs >= 0.5).float()
      accuracy = (preds == y_true).mean().item()
      f1 = f1_score(y_true, preds)
      # r2 = r2_score(y_true, preds)
      # rmse = root_mean_squared_error(y_true, preds)
      test_accuracies.append(accuracy)
      f1_scores.append(f1)
      # r2_scores.append(r2)
      # rmses.append(rmse)


    if (epoch+1) % (epochs / 10) == 0:
      print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.3f}, F1: {f1:.3f}")

  with torch.no_grad():
    probs = torch.sigmoid(model(X_test))
    preds = (probs >=0.5).int().cpu().numpy() if torch.cuda.is_available() else (probs >= 0.5).float()
    
    y_true = y_test.int().cpu().numpy() if torch.cuda.is_available() else y_tst_tensor

  print(classification_report(y_true, preds))
  
  return train_losses, test_accuracies, f1_scores

def find_importances(model, X, y):
  wrapped_model = TorchModelWrapper(model=model, device=device, task="classification")

  # X_nn is the dataframe of X converted to a numpy object
  result = permutation_importance(
    wrapped_model,
    X.to_numpy(), y.to_numpy(),
    n_repeats=10,
    random_state=117,
    n_jobs=-1
  )

  importances = result.importances_mean
  std_devs = result.importances_std
  indices = np.argsort(importances)[::-1]

  with torch.no_grad():
    probs = torch.sigmoid(model(X_tst_tensor))
    preds = (probs >=0.5).int().cpu().numpy() if torch.cuda.is_available() else (probs >= 0.5).float()
    
    y_true = y_tst_tensor.int().cpu().numpy() if torch.cuda.is_available() else y_tst_tensor

  feat_import = pl.DataFrame({"Feature": np.array(X.columns)[indices], "Importance": importances[indices], "Std": std_devs[indices]})

  print(classification_report(y_true, preds))

  return feat_import

# To use integrated gradients, the model needs to be differential
def integrated_gradients(model, inputs, baseline=None, steps=50):
  if baseline is None:
    baseline = torch.zeros_like(inputs)

  inputs = inputs.requires_grad_()
  scaled_inputs = [baseline + (float(i) / steps) * (inputs - baseline) for i in range(steps + 1)]
  grad = []

  for i in scaled_inputs:
    model.zero_grad()
    out = model(i)
    out = out.squeeze()
    out.backward(troch.ones_like(out))
    grads.append(i.grad.detach().clone())
    i.grad.zero_()

  avg_grads = torch.stack(grads).mean(dim=0)
  integrated_grads = (inputs - baseline) * avg_grads
  return integrated_grads

if (torch.cuda.is_available()):
  print(f"CUDA is available, using the GPU")
  print(torch.cuda.get_device_name(0))
  device = torch.device("cuda")
  torch.cuda.manual_seed(117)
else:
  print(f"CUDA is not available, using the CPU")
  device = torch.device("cpu")
  torch.manual_seed(117)

X_df = df.drop(["parcel", "before1980", "yrbuilt"])
y_df = df.select(["before1980"])

X = X_df.to_numpy()
y = y_df.to_numpy()


scaler = StandardScaler()
X = scaler.fit_transform(X)

X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=117)

X_trn_tensor = torch.tensor(X_trn, dtype=torch.float32)
y_trn_tensor = torch.tensor(y_trn, dtype=torch.float32).view(-1, 1)
X_tst_tensor = torch.tensor(X_tst, dtype=torch.float32)
y_tst_tensor = torch.tensor(y_tst, dtype=torch.float32).view(-1, 1)

if torch.cuda.is_available():
  print(f"Moving Tensors to the GPU")
  X_trn_tensor = X_trn_tensor.to(device)
  y_trn_tensor = y_trn_tensor.to(device)
  X_tst_tensor = X_tst_tensor.to(device)
  y_tst_tensor = y_tst_tensor.to(device) 
```


## QUESTION 1

__Create 2-3 charts that evaluate the relationships between each of the top 2 or 3 most important variables (as found in Unit 4 Task 2) and the year the home was built.__ Describe what you learn from the charts about how that variable is related to year built.   

_type your write-up and analysis here_
_For example, how does a particular feature determine whether a house is built before 1980 or not. Does having a garage mean the house is before 1980._

```{python}
# Include and execute your code here
num_epochs = 1000
epochs = range(num_epochs)
shallow_mdl = Neural_Net(input_size=X.shape[1])
deep_mdl = Neural_Net(input_size=X.shape[1], shallow=False)

if torch.cuda.is_available():
  print(f"Moving the models to the GPU")
  shallow_mdl = shallow_mdl.to(device)
  deep_mdl = deep_mdl.to(device)


loss_shallow, acc_shallow, f1_shallow = train_model(shallow_mdl, "Shallow", X_trn_tensor, X_tst_tensor, y_trn_tensor, y_tst_tensor, epochs=num_epochs)
loss_deep, acc_deep, f1_deep = train_model(deep_mdl, "Deep", X_trn_tensor, X_tst_tensor, y_trn_tensor, y_tst_tensor, epochs=num_epochs)

res = pl.DataFrame({
  "epoch": epochs,
  "loss_shallow": loss_shallow,
  "loss_deep": loss_deep,
  "acc_shallow": acc_shallow,
  "acc_deep": acc_deep,
  "f1_shallow": f1_shallow,
  "f1_deep": f1_deep
})

kf = KFold(n_splits=5, shuffle=True, random_state=117)

shall_losses = np.zeros(num_epochs)
shall_accs = np.zeros(num_epochs)
shall_f1 = np.zeros(num_epochs)
deep_losses = np.zeros(num_epochs)
deep_accs = np.zeros(num_epochs)
deep_f1 = np.zeros(num_epochs)

for fold, (train_idx, val_idx) in enumerate(kf.split(X_trn_tensor)):
  print(f"Fold {fold + 1}")
  X_train = X_trn_tensor[train_idx]
  X_val = X_trn_tensor[val_idx]
  y_train = y_trn_tensor[train_idx]
  y_val = y_trn_tensor[val_idx]

  shall_model = Neural_Net(input_size=X_train.shape[1])
  deep_model = Neural_Net(input_size=X_train.shape[1])

  if torch.cuda.is_available():
    shall_model = shall_model.to(device)
    deep_model = deep_model.to(device)

  s_loss, s_acc, s_f1 = train_model(shall_model, "Shallow", X_train, X_val, y_train, y_val, num_epochs)
  shall_losses += np.array(s_loss)
  shall_accs += np.array(s_acc)
  shall_f1 += np.array(s_f1)

  d_loss, d_acc, d_f1 = train_model(deep_model, "Deep", X_train, X_val, y_train, y_val, num_epochs)
  deep_losses += np.array(d_loss)
  deep_accs += np.array(d_acc)
  deep_f1 += np.array(d_f1)

shall_losses /= 5
shall_accs /= 5
shall_f1 /= 5
deep_losses /= 5
deep_accs /= 5
deep_f1 /= 5

cv_res = pl.DataFrame({
  "epoch": epochs,
  "loss_shallow": shall_losses,
  "loss_deep": deep_losses,
  "accuracy_shallow": shall_accs,
  "accuracy_deep": deep_accs,
  "f1_shallow": shall_f1,
  "f1_deep": deep_f1
})
```

```{python}
loss_unpivot = res.unpivot(index='epoch', on=['loss_shallow', 'loss_deep'], variable_name='mdl', value_name='value')
acc_f1_unpivot = res.unpivot(index='epoch', on=['acc_shallow', 'acc_deep', 'f1_shallow', 'f1_deep'], variable_name='mdl', value_name='value')

loss_graph = (
    ggplot(data=loss_unpivot)
    + geom_line(mapping = aes(x = 'epoch', y = 'value', color='mdl'))
    + labs(
      title="Loss Over Time",
      x="Epochs",
      y="Loss",
      color="Model"
    )
    + theme(
        panel_background=element_rect(fill='gray'),
        plot_background=element_rect(fill='gray'),
        panel_grid_major=element_rect(fill='gray'),
        legend_background=element_rect(fill='gray'),
        axis_text=element_text(color='white'),
        axis_title=element_text(color='white'),
        plot_title=element_text(color='white'),
        plot_subtitle=element_text(color='white'),
        legend_text=element_text(color='white'),
        legend_title=element_text(color='white'),
        label_text=element_text(color='white')
    )
    + ggsize(1600, 900)
)

acc_f1_graph = (
  ggplot(data=acc_f1_unpivot)
  + geom_line(mapping = aes(x = 'epoch', y = 'value', color='mdl'), size=1.5)
  + labs(
    title="Accuracy and F1 Score Over Time",
    x="Epochs",
    y="Accuracy",
    color="Model"
  )
  + theme(
      panel_background=element_rect(fill='gray'),
      plot_background=element_rect(fill='gray'),
      panel_grid_major=element_rect(fill='gray'),
      legend_background=element_rect(fill='gray'),
      axis_text=element_text(color='white'),
      axis_title=element_text(color='white'),
      plot_title=element_text(color='white'),
      plot_subtitle=element_text(color='white'),
      legend_text=element_text(color='white'),
      legend_title=element_text(color='white'),
      label_text=element_text(color='white')
  )
  + ggsize(1600, 900)
)
display(loss_graph)
display(acc_f1_graph)
```

```{python}
cv_loss_unpivot = cv_res.unpivot(index='epoch', on=['loss_shallow', 'loss_deep'], variable_name='mdl', value_name='value')
cv_acc_f1_unpivot = cv_res.unpivot(index='epoch', on=['accuracy_shallow', 'accuracy_deep', 'f1_shallow', 'f1_deep'], variable_name='mdl', value_name='value')

cv_loss_graph = (
    ggplot(data=cv_loss_unpivot)
    + geom_line(mapping = aes(x = 'epoch', y = 'value', color='mdl'))
    + labs(
      title="Cross Validation Loss Over Time",
      x="Epochs",
      y="Loss",
      color="Model"
    )
    + theme(
        panel_background=element_rect(fill='gray'),
        plot_background=element_rect(fill='gray'),
        panel_grid_major=element_rect(fill='gray'),
        legend_background=element_rect(fill='gray'),
        axis_text=element_text(color='white'),
        axis_title=element_text(color='white'),
        plot_title=element_text(color='white'),
        plot_subtitle=element_text(color='white'),
        legend_text=element_text(color='white'),
        legend_title=element_text(color='white'),
        label_text=element_text(color='white')
    )
    + ggsize(1600, 900)
)

cv_acc_f1_graph = (
  ggplot(data=cv_acc_f1_unpivot)
  + geom_line(mapping = aes(x = 'epoch', y = 'value', color='mdl'), size=1.5)
  + labs(
    title="Cross Validation Accuracy and F1 Score Over Time",
    x="Epochs",
    y="Accuracy",
    color="Model"
  )
  + theme(
      panel_background=element_rect(fill='gray'),
      plot_background=element_rect(fill='gray'),
      panel_grid_major=element_rect(fill='gray'),
      legend_background=element_rect(fill='gray'),
      axis_text=element_text(color='white'),
      axis_title=element_text(color='white'),
      plot_title=element_text(color='white'),
      plot_subtitle=element_text(color='white'),
      legend_text=element_text(color='white'),
      legend_title=element_text(color='white'),
      label_text=element_text(color='white')
  )
  + ggsize(1600, 900)
)
display(cv_loss_graph)
display(cv_acc_f1_graph)
```

_A possible method to evaluate the relationship of each variable is a table. One column is the list of features, another column is the count of houses before 1980 and another column of the count of houses after 1980. Fead the entire dataset into the model and get the predictions. Just count the predictions._

```{python}
res_shallow = find_importances(shallow_mdl, X_df, y_df)
res_deep = find_importances(deep_mdl, X_df, y_df)
# Example for a single test input
# input_tensor = torch.tensor(X_test[0], dtype=torch.float32)
# attr = integrated_gradients(model, input_tensor.unsqueeze(0))
# print(f"Integrated gradients for input 0: {attr}")
```

```{python}
if torch.cuda.is_available():
  top_shallow_feats = res_shallow.sort("Importance").tail(10)
  top_deep_feats = res_deep.sort("Importance").tail(10)
  feats_bar_shallow = (
    ggplot(data=top_shallow_feats)
        + geom_bar(mapping=aes(x='Importance', y='Feature', fill='Feature', color='Feature'), stat='identity')
        + guides(color="none")
        + labs(
          title="Feature Importance of Shallow Model",
          subtitle="The top 10 most important features are shown that helped the model learn on the data.",
          x="Importance",
          y="Feature",
          fill='Feature'
        )
        + theme(
            panel_background=element_rect(fill='gray'),
            plot_background=element_rect(fill='gray'),
            panel_grid_major=element_rect(fill='gray'),
            legend_background=element_rect(fill='gray'),
            axis_text=element_text(color='white'),
            axis_title=element_text(color='white'),
            plot_title=element_text(color='white'),
            plot_subtitle=element_text(color='white'),
            legend_text=element_text(color='white'),
            legend_title=element_text(color='white'),
            label_text=element_text(color='white')
        )
        + ggsize(1600, 900)
  )

  feats_bar_deep = (
    ggplot(data=top_deep_feats)
        + geom_bar(mapping=aes(x='Importance', y='Feature', fill='Feature', color='Feature'), stat='identity')
        + guides(color="none")
        + labs(
          title="Feature Importance of Deep Model",
          subtitle="The top 10 most important features are shown that helped the model learn on the data.",
          x="Importance",
          y="Feature",
          fill='Feature'
        )
        + theme(
            panel_background=element_rect(fill='gray'),
            plot_background=element_rect(fill='gray'),
            panel_grid_major=element_rect(fill='gray'),
            legend_background=element_rect(fill='gray'),
            axis_text=element_text(color='white'),
            axis_title=element_text(color='white'),
            plot_title=element_text(color='white'),
            plot_subtitle=element_text(color='white'),
            legend_text=element_text(color='white'),
            legend_title=element_text(color='white'),
            label_text=element_text(color='white')
        )
        + ggsize(1600, 900)
  )

  display(feats_bar_shallow)
  display(feats_bar_deep)
else:
  print("CUDA is not available!")
```

```{python}
# SHAP method
# Wrap model prediction function
def model_predict(x):
    model.eval()
    with torch.no_grad():
        logits = model(torch.tensor(x, dtype=torch.float32)).squeeze()
        return torch.sigmoid(logits).numpy()

explainer = shap.DeepExplainer(model, torch.tensor(X_train, dtype=torch.float32))
shap_values = explainer.shap_values(torch.tensor(X_test, dtype=torch.float32))

shap.summary_plot(shap_values, X_test, feature_names=["feature1", "feature2"])

```

```{python}
# Integrated gradients method
input_tensor = torch.tensor(X_test[0], dtype=torch.float32)
attr = integrated_gradients(model, input_tensor.unsqueeze(0))
print(f"Integrated gradients for input 0: {attr}")
```

## QUESTION 2

__Create at least one other chart to examine a variable(s) you thought might be important but apparently was not. The chart should show its relationship to the year built.__ Describe what you learn from the chart about how that variable is related to year built. Explain why you think it was not (very) important in the model.

_type your write-up and analysis here_

```{python}
# Include and execute your code here


```